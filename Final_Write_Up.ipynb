{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Write Up.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ioEyl3XCog8V",
        "Yvnn1lS6v0XH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5MxLegmpuxN",
        "colab_type": "text"
      },
      "source": [
        "# LS 123 Data Analysis Project (Spring 2019)\n",
        "### by David Jeong & Bowie Lam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEyl3XCog8V",
        "colab_type": "text"
      },
      "source": [
        "# Research, Data Applicability, and Data Analysis Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yIZhogHuU3h",
        "colab_type": "text"
      },
      "source": [
        "**Questions:**\n",
        "* Can we predict if race influences whether or not a person reports a crime based on data from other victims?\n",
        "* Can we predict who is a likely victim given the type of crime, injury, and various other features?\n",
        "* Are certain races more prone to certain types of crime and does income have an impact on the types of crime that victims experience?\n",
        "\n",
        "\n",
        "**Motivation:**\n",
        "* We wanted to explore this topic because we personally have African American friends who are victimized but do not report these instances because they don't believe in the justice and/or the legal system. They often feel as if little to nothing will be done, and that it happens too often to care. Therefore, wanted to see if there is a relationship between race and unreport crimes, and if machine learning will detect anything different. In addition, It's also important to examine what other factors or features may prevent a victim from reporting a crime or not. Unreported crimes allow criminals to get away free, thus not held accountable for their actions. And victims are not proetcted from this injustice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coDwFjhfeUHU",
        "colab_type": "text"
      },
      "source": [
        "**Note:** You may need to install eli5 using \"pip install eli5\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8UdRmT5bvNq",
        "colab_type": "code",
        "outputId": "b394a13e-69ca-443a-af50-20e326b980cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "!pip install eli5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting eli5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/2f/c85c7d8f8548e460829971785347e14e45fa5c6617da374711dec8cb38cc/eli5-0.10.1-py2.py3-none-any.whl (105kB)\n",
            "\r\u001b[K     |███                             | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from eli5) (0.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.21.3)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from eli5) (0.8.5)\n",
            "Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (19.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from eli5) (1.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from eli5) (2.10.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from eli5) (1.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from eli5) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->eli5) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->eli5) (1.1.1)\n",
            "Installing collected packages: eli5\n",
            "Successfully installed eli5-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnXRVpe4cjms",
        "colab_type": "code",
        "outputId": "20941098-dd1d-48eb-b370-cc784b84a749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "# load all libraries \n",
        "\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xgboost as xgbb\n",
        "import eli5\n",
        "\n",
        "#matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#scikit-learn\n",
        "from datetime import datetime\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import metrics\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.metrics import roc_curve, auc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL1YX4H2ePnr",
        "colab_type": "code",
        "outputId": "58cd59c8-8e8e-42a5-addb-cc416a821b17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq0WIDsduvsw",
        "colab_type": "text"
      },
      "source": [
        "**About the Data:**\n",
        "* The data cromes from the National Crime Victimization Survey, which has been collected by the U.S. Census Bureau since 1973. It's collected biannually from a large sample of households totaling over at least one hundred thousand, therefore there's a large sample size to work with. We chose this dataset mainly because it's been officially cleaned and all features are strategically mapped to numerical values. This form of quantitative data is easy to replicate, comparable, and can be statistically analyzed. In addition, this survey is the primary source of information on criminal victimization, and the only source that has data on crimes that are not reported to law enforcement, therefore it's a credible dataset to rely on and utilize.\n",
        "\n",
        "\n",
        "* Variables/Features in the data includes information on the crime itself, such as the type, year, location, and region of the incident, if there was a weapon involved (if yes, then what category does it fall under), if there was an injury (if yes, then was medical treatment provided), description/category of the victimization, etc. There's basic demographic information about the respondent, such as age, household income, location of residence (urban, suburban, or rural), marital status, race, and sex. Lastly, there's information on the relationship between the offender and the victim, which ranges from \"intimiate\" to \"do not know number of offenders\" and the data also indicates whether or not the crime was reported to police, and reasons the crime was or was not reported.\n",
        "*Each row represents one household surveyed and each column represents different questions and the respondents’ answers to them.\n",
        "\n",
        "\n",
        "* Download the 2008-2017 Personal Victimization Dataset from this link: https://www.bjs.gov/developer/ncvs/index.cfm\n",
        "* View the codebook for the dataset from this link: https://www.bjs.gov/developer/ncvs/personalFields.cfm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLfK9lpmebSN",
        "colab_type": "code",
        "outputId": "e6bd9448-8ab9-4ea3-d6f9-c2f04b17d718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "# download the data\n",
        "data = pd.read_csv('gdrive/My Drive/Colab Notebooks/NCVS_PERSONAL_VICTIMIZATION_2008-2017.csv')\n",
        "data_num = data.copy(deep=True)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>weight</th>\n",
              "      <th>gender</th>\n",
              "      <th>race1r</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>ethnic1r</th>\n",
              "      <th>ager</th>\n",
              "      <th>marital2</th>\n",
              "      <th>hincome</th>\n",
              "      <th>popsize</th>\n",
              "      <th>region</th>\n",
              "      <th>msa</th>\n",
              "      <th>direl</th>\n",
              "      <th>notify</th>\n",
              "      <th>weapon</th>\n",
              "      <th>weapcat</th>\n",
              "      <th>newcrime</th>\n",
              "      <th>newoff</th>\n",
              "      <th>seriousviolent</th>\n",
              "      <th>injury</th>\n",
              "      <th>treatment</th>\n",
              "      <th>vicservices</th>\n",
              "      <th>locationr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008</td>\n",
              "      <td>4828.52379</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008</td>\n",
              "      <td>3087.88814</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008</td>\n",
              "      <td>3302.31117</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008</td>\n",
              "      <td>6796.96420</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008</td>\n",
              "      <td>3758.85256</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year      weight  gender  race1r  ...  injury  treatment  vicservices  locationr\n",
              "0  2008  4828.52379       2       1  ...       0        0.0          2.0          4\n",
              "1  2008  3087.88814       1       1  ...       0        0.0          1.0          1\n",
              "2  2008  3302.31117       2       1  ...       0        0.0          2.0          1\n",
              "3  2008  6796.96420       2       2  ...       0        0.0          2.0          4\n",
              "4  2008  3758.85256       2       1  ...       0        0.0          2.0          5\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27GgUn66f5Io",
        "colab_type": "code",
        "outputId": "c46c58e1-465f-4abf-8d00-e7952ff8f77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# show the column labels for the dataframe\n",
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['year', 'weight', 'gender', 'race1r', 'hispanic', 'ethnic1r', 'ager',\n",
              "       'marital2', 'hincome', 'popsize', 'region', 'msa', 'direl', 'notify',\n",
              "       'weapon', 'weapcat', 'newcrime', 'newoff', 'seriousviolent', 'injury',\n",
              "       'treatment', 'vicservices', 'locationr'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "466472ea-a8da-48b1-d54f-3aab1b81fffa",
        "id": "rAApMR2nhJdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# group by the location of residence (msa) then race (race1r)\n",
        "data.groupby(['msa', 'race1r']).count()['year']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "msa  race1r\n",
              "1    1         4566\n",
              "     2         1292\n",
              "     3          616\n",
              "2    1         5721\n",
              "     2          684\n",
              "     3          512\n",
              "3    1         1833\n",
              "     2          139\n",
              "     3          124\n",
              "Name: year, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Xm4Ga3DhJMl"
      },
      "source": [
        "**Note:**\n",
        "* \"msa\" key:\n",
        "  * 1 =\tUrban\n",
        "  * 2 = Suburban\n",
        "  * 3 =\tRural \n",
        "* \"race1r\" key:\n",
        "  * 1 = white\n",
        "  * 2 = black\n",
        "  * 3 = other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA3NRGS3inpF",
        "colab_type": "code",
        "outputId": "d48b9d44-1f39-487b-f935-7d2134413328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "# plot a bar graph showing how many households recorded for each race\n",
        "race_count = data['race1r'].value_counts()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(race_count.index, race_count.values)\n",
        "plt.title('Number of Races Recorded')\n",
        "plt.ylabel('Number of Households', fontsize=12)\n",
        "plt.xlabel('Race', fontsize=12)\n",
        "plt.xticks([0,1,2],['White', 'Black', 'Other'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAHzCAYAAABL6JtLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf8F5dp3kkd7",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* From the bar graph above, it illustrates is an immense imbalance in the number of responses received from the different races (white, black, and other). This could be due to the regions in which the surveyors chose to survey, such as picking regions that are populated by more white households than blacks or others. Or, this discrepency could be due to the willingness of whites as a culture to participate in surveys compared to blacks and others because they view it as something beneficial to studies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqKRtSEYpw2_",
        "colab_type": "code",
        "outputId": "3e6fb648-aaa7-49b8-9ab2-89099654a1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "# plot a bar graph showing the number of households recorded for each population size\n",
        "pop_count = data['popsize'].value_counts()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.barplot(pop_count.index, pop_count.values)\n",
        "plt.title('Number of Population Size')\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Population Size', fontsize=12)\n",
        "plt.xticks(np.arange(6))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAHzCAYAAAByjyoQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqQOGNkmSpA78f3Mev6s5WlPoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZVBklrTqvfh",
        "colab_type": "code",
        "outputId": "27a1e6b0-512e-435c-efdb-e8d5a44abcd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "# descriptions for the x-axis values for the bar graph above\n",
        "popsize = pd.DataFrame()\n",
        "popsize['Description'] = ['Not a place', '100,000 or fewer', '100,000 to 249,999', '250,000 to 499,999', '500,000 to 999,999', '1 million or more']\n",
        "popsize.index.rename('Value', inplace=True)\n",
        "popsize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Value</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not a place</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100,000 or fewer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100,000 to 249,999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>250,000 to 499,999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500,000 to 999,999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1 million or more</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Description\n",
              "Value                    \n",
              "0             Not a place\n",
              "1        100,000 or fewer\n",
              "2      100,000 to 249,999\n",
              "3      250,000 to 499,999\n",
              "4      500,000 to 999,999\n",
              "5       1 million or more"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaGY0IsspArI",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* From the bar graph above, it illustrates that most of the households were located in cities or areas with a population of 100,000 or fewer.\n",
        "* The size range for the place in which the housing unit is located. \"Not a place\" is a concentration of population that is either not legally bounded as an incorporated place having an active government or not delineated for statistical purposes as a census designated place with definite geographic boundaries, such as a city, town, or village."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KIdARRNo5DK",
        "colab_type": "code",
        "outputId": "71dc9726-979b-4b62-b93e-7233fef6f340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# return the number of households for each location residence\n",
        "data['msa'].value_counts(sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    6474\n",
              "2    6917\n",
              "3    2096\n",
              "Name: msa, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVm6acRf4RoV",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* 1 =\tUrban\n",
        "* 2 = Suburban\n",
        "* 3 =\tRural "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbOfiG6ZsrDy",
        "colab_type": "code",
        "outputId": "643668be-8b58-4327-c6f0-82d4eae2b44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# return the number of households in each population size for each race\n",
        "data.groupby(['msa', 'popsize']).count()['year']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "msa  popsize\n",
              "1    0            17\n",
              "     1          1516\n",
              "     2          1441\n",
              "     3          1106\n",
              "     4          1177\n",
              "     5          1217\n",
              "2    0          2227\n",
              "     1          4285\n",
              "     2           405\n",
              "3    0           755\n",
              "     1          1341\n",
              "Name: year, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oYtGesWs0mY",
        "colab_type": "code",
        "outputId": "6a56e035-cf6d-4334-f602-7989343f3969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# group by the year (year) then whether or not the crime was reported to police (notify)\n",
        "# then count cells for each\n",
        "temp = data.drop(data[data['notify'] == 8].index)\n",
        "temp = temp.drop(data[data['notify'] == 3].index)\n",
        "temp1 = temp.groupby(['year', 'notify']).count()[['weight']]\n",
        "total = temp.groupby('year').count()['notify']\n",
        "temp1['weight'] = temp1['weight'] / total\n",
        "temp1.rename(columns={'weight':'percentage'}, inplace=True)\n",
        "temp1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>percentage</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <th>notify</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2008</th>\n",
              "      <th>1</th>\n",
              "      <td>0.483138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.516862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2009</th>\n",
              "      <th>1</th>\n",
              "      <td>0.500806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.499194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2010</th>\n",
              "      <th>1</th>\n",
              "      <td>0.527482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.472518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2011</th>\n",
              "      <th>1</th>\n",
              "      <td>0.494883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.505117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2012</th>\n",
              "      <th>1</th>\n",
              "      <td>0.480024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.519976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2013</th>\n",
              "      <th>1</th>\n",
              "      <td>0.468685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.531315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2014</th>\n",
              "      <th>1</th>\n",
              "      <td>0.500740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.499260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2015</th>\n",
              "      <th>1</th>\n",
              "      <td>0.473420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.526580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2016</th>\n",
              "      <th>1</th>\n",
              "      <td>0.466737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.533263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">2017</th>\n",
              "      <th>1</th>\n",
              "      <td>0.456242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.543758</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             percentage\n",
              "year notify            \n",
              "2008 1         0.483138\n",
              "     2         0.516862\n",
              "2009 1         0.500806\n",
              "     2         0.499194\n",
              "2010 1         0.527482\n",
              "     2         0.472518\n",
              "2011 1         0.494883\n",
              "     2         0.505117\n",
              "2012 1         0.480024\n",
              "     2         0.519976\n",
              "2013 1         0.468685\n",
              "     2         0.531315\n",
              "2014 1         0.500740\n",
              "     2         0.499260\n",
              "2015 1         0.473420\n",
              "     2         0.526580\n",
              "2016 1         0.466737\n",
              "     2         0.533263\n",
              "2017 1         0.456242\n",
              "     2         0.543758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmq04b3Dfksd",
        "colab_type": "code",
        "outputId": "ab70c649-efbc-45fd-c760-5749a9fa9d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "temp1.unstack().plot(kind='bar',stacked=True, figsize=(10,7))\n",
        "plt.ylabel('percentage')\n",
        "plt.title('Percentage of Notifying Police')\n",
        "plt.legend(['yes', 'no'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHJCAYAAADNUu5VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpgf8PuwesCE3QjRUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hhlQ_PDcSEw",
        "colab_type": "text"
      },
      "source": [
        "**Note** \n",
        "* Here we wanted to see if there was a difference between the years on the rate at which victims reported the crimes. It seemed like the rate was relatively similar throughout the years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU1EoTxlvscc",
        "colab_type": "text"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvnn1lS6v0XH",
        "colab_type": "text"
      },
      "source": [
        "##Cleaning the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erSx6Vj6vzBK",
        "colab_type": "code",
        "outputId": "4e84f606-0d98-4470-dde3-411264c880f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download the data\n",
        "data = pd.read_csv('gdrive/My Drive/Colab Notebooks/NCVS_PERSONAL_VICTIMIZATION_2008-2017.csv')\n",
        "data_num = data.copy(deep=True)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>weight</th>\n",
              "      <th>gender</th>\n",
              "      <th>race1r</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>ethnic1r</th>\n",
              "      <th>ager</th>\n",
              "      <th>marital2</th>\n",
              "      <th>hincome</th>\n",
              "      <th>popsize</th>\n",
              "      <th>region</th>\n",
              "      <th>msa</th>\n",
              "      <th>direl</th>\n",
              "      <th>notify</th>\n",
              "      <th>weapon</th>\n",
              "      <th>weapcat</th>\n",
              "      <th>newcrime</th>\n",
              "      <th>newoff</th>\n",
              "      <th>seriousviolent</th>\n",
              "      <th>injury</th>\n",
              "      <th>treatment</th>\n",
              "      <th>vicservices</th>\n",
              "      <th>locationr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008</td>\n",
              "      <td>4828.52379</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008</td>\n",
              "      <td>3087.88814</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008</td>\n",
              "      <td>3302.31117</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008</td>\n",
              "      <td>6796.96420</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008</td>\n",
              "      <td>3758.85256</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>88.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year      weight  gender  race1r  ...  injury  treatment  vicservices  locationr\n",
              "0  2008  4828.52379       2       1  ...       0        0.0          2.0          4\n",
              "1  2008  3087.88814       1       1  ...       0        0.0          1.0          1\n",
              "2  2008  3302.31117       2       1  ...       0        0.0          2.0          1\n",
              "3  2008  6796.96420       2       2  ...       0        0.0          2.0          4\n",
              "4  2008  3758.85256       2       1  ...       0        0.0          2.0          5\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fksS2n5Yv_JK",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* All of the categorical data were encoded for us so we did not have to do any encoding. A codebook is provided for the numericalized data, which can be found at this link: https://www.bjs.gov/developer/ncvs/personalFields.cfm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSHGNSKwvrdg",
        "colab_type": "code",
        "outputId": "5eab41dd-542e-4cdc-e09b-5966b40a29c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# generate descriptive statistics, excluding NaN values\n",
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>weight</th>\n",
              "      <th>gender</th>\n",
              "      <th>race1r</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>ethnic1r</th>\n",
              "      <th>ager</th>\n",
              "      <th>marital2</th>\n",
              "      <th>hincome</th>\n",
              "      <th>popsize</th>\n",
              "      <th>region</th>\n",
              "      <th>msa</th>\n",
              "      <th>direl</th>\n",
              "      <th>notify</th>\n",
              "      <th>weapon</th>\n",
              "      <th>weapcat</th>\n",
              "      <th>newcrime</th>\n",
              "      <th>newoff</th>\n",
              "      <th>seriousviolent</th>\n",
              "      <th>injury</th>\n",
              "      <th>treatment</th>\n",
              "      <th>vicservices</th>\n",
              "      <th>locationr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>14957.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.00000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "      <td>15467.000000</td>\n",
              "      <td>15289.000000</td>\n",
              "      <td>15487.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2012.938142</td>\n",
              "      <td>3771.068377</td>\n",
              "      <td>1.512107</td>\n",
              "      <td>1.298250</td>\n",
              "      <td>1.949893</td>\n",
              "      <td>1.720217</td>\n",
              "      <td>5.085878</td>\n",
              "      <td>2.412733</td>\n",
              "      <td>22.919168</td>\n",
              "      <td>1.610706</td>\n",
              "      <td>2.722671</td>\n",
              "      <td>1.717311</td>\n",
              "      <td>3.299606</td>\n",
              "      <td>1.566992</td>\n",
              "      <td>1.870278</td>\n",
              "      <td>0.872861</td>\n",
              "      <td>1.02783</td>\n",
              "      <td>3.467231</td>\n",
              "      <td>1.686124</td>\n",
              "      <td>0.240008</td>\n",
              "      <td>0.343506</td>\n",
              "      <td>1.919158</td>\n",
              "      <td>2.424227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.944323</td>\n",
              "      <td>5082.286911</td>\n",
              "      <td>0.499870</td>\n",
              "      <td>0.609102</td>\n",
              "      <td>2.957064</td>\n",
              "      <td>1.113729</td>\n",
              "      <td>1.884975</td>\n",
              "      <td>5.866953</td>\n",
              "      <td>34.841688</td>\n",
              "      <td>1.472069</td>\n",
              "      <td>1.020767</td>\n",
              "      <td>0.688102</td>\n",
              "      <td>1.237408</td>\n",
              "      <td>0.679787</td>\n",
              "      <td>0.531118</td>\n",
              "      <td>1.581663</td>\n",
              "      <td>0.16449</td>\n",
              "      <td>0.923215</td>\n",
              "      <td>0.520610</td>\n",
              "      <td>0.427102</td>\n",
              "      <td>0.659165</td>\n",
              "      <td>0.272602</td>\n",
              "      <td>1.320979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2008.000000</td>\n",
              "      <td>59.821060</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2010.000000</td>\n",
              "      <td>1945.657310</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2013.000000</td>\n",
              "      <td>3070.250890</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2016.000000</td>\n",
              "      <td>3909.655110</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2017.000000</td>\n",
              "      <td>153486.647700</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               year         weight  ...   vicservices     locationr\n",
              "count  15487.000000   15487.000000  ...  15289.000000  15487.000000\n",
              "mean    2012.938142    3771.068377  ...      1.919158      2.424227\n",
              "std        2.944323    5082.286911  ...      0.272602      1.320979\n",
              "min     2008.000000      59.821060  ...      1.000000      1.000000\n",
              "25%     2010.000000    1945.657310  ...      2.000000      1.000000\n",
              "50%     2013.000000    3070.250890  ...      2.000000      3.000000\n",
              "75%     2016.000000    3909.655110  ...      2.000000      3.000000\n",
              "max     2017.000000  153486.647700  ...      2.000000      5.000000\n",
              "\n",
              "[8 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWeEHzRywEfr",
        "colab_type": "code",
        "outputId": "6e879939-7d56-44cf-cce4-9ed5f7cf3713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# return the number of households for police notification\n",
        "data['notify'].value_counts(sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8      70\n",
              "1    7333\n",
              "2    7877\n",
              "3     207\n",
              "Name: notify, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IswbTFnswd_5",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* key:\n",
        "  * 1 = Yes, reported to the police\n",
        "  * 2 = No, did not report to the police\n",
        "  * 3 = Do not know\n",
        "* The value 8 was not present in the codebook so we decided to omit those rows entirely. Also we noticed that there was a huge class imbalance because not even 10% of the responses included the \"Do not know\" response so we decided to remove those rows as well to make this a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4tmKC8fwbDF",
        "colab_type": "code",
        "outputId": "828520a4-f697-4662-f35b-efef2a40c844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find the count of hispanic origin and not, without considering race\n",
        "data['hispanic'].value_counts(sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88       18\n",
              "1      2324\n",
              "2     13145\n",
              "Name: hispanic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8sTAWJP-cM5",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* Key:\n",
        "  * 1 = Hispanic\n",
        "  * 2 = Non-Hispanic\n",
        "* The value 88 was not assigned a value in the codebook, therefore we removed all rows with 88 as the value for the \"hispanic\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk3f-aNXwzGD",
        "colab_type": "code",
        "outputId": "faf9f987-e82e-49d3-d1b6-b8d8248b9e9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find the count of each marital status\n",
        "data['marital2'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     7452\n",
              "2     4211\n",
              "4     2467\n",
              "5      844\n",
              "3      444\n",
              "88      69\n",
              "Name: marital2, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9HzDLPw6tI",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* Key: \n",
        "  * 1\t= Never married\n",
        "  * 2\t= Married\n",
        "  * 3= \tWidowed\n",
        "  * 4\t= Divorced\n",
        "  * 5 =\tSeparated\n",
        "* In the codebook, there were no values assigned for 88 other than household income so we removed all the rows with 88 as the values for the \"marital2\" column. \n",
        "* We also dropped the column \"Weight\" entirely because it's not related to the race of the victim or the type of crime experienced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZNQEdhww09x",
        "colab_type": "code",
        "outputId": "dc94826e-2e2e-4f40-e7af-3cceee45e291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# find count for each household income bracket\n",
        "data['hincome'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.0    3325\n",
              "7.0     2286\n",
              "6.0     1775\n",
              "3.0     1710\n",
              "5.0     1683\n",
              "2.0     1510\n",
              "4.0     1439\n",
              "1.0     1229\n",
              "Name: hincome, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZgOR22fzNC8",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* Key: (all amounts are in U.S. Dollars)\n",
        "  * 1\t= Less than 7,500\n",
        "  * 2 = 7,500 to 14,999\n",
        "  * 3\t= 15,000 to 24,999\n",
        "  * 4\t= 25,000 to 34,999\n",
        "  * 5\t= 35,000 to 49,999\n",
        "  * 6\t= 50,000 to 74,999\n",
        "  * 7\t= 75,000 or more\n",
        "  * 88\t= Unknown\n",
        "* We noticed that there were a large number of households with unknown income and we thought it would be best to remove them to reduce unncessary noise.\n",
        "* The total income of the household head and all members of the household for the 12 months preceding the interview. Includes wages, salaries, net income from businesses or farms, pensions, interest, dividends, rent, and any other form of monetary income."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK9kDU7BzFK-",
        "colab_type": "code",
        "outputId": "c504c61c-56b4-4f74-e2f4-ed7410a7c283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list88 = []\n",
        "for col in data.columns:\n",
        "  if max(data[col]) == 88:\n",
        "    list88.append(col)\n",
        "list88"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hispanic', 'marital2', 'hincome']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSiXG7Bx1J",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "* Lastly, we decided to drop the weight column as it was a redundant feature that was just derived from the population size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AICsZTkXzIEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we dropped the rows as discussed above.\n",
        "data = data.drop(data[data['notify'] == 8].index)\n",
        "data = data.drop(data[data['notify'] == 3].index)\n",
        "data = data.drop(['weight'], axis=1)\n",
        "data = data.drop(data[data['hincome'] == 88].index)\n",
        "data = data.drop(data[data['marital2'] == 88].index)\n",
        "data = data.drop(data[data['hispanic'] == 88].index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MVrkJ4I_C8a",
        "colab_type": "code",
        "outputId": "4bb3242e-2f1d-4445-db21-d37e8ed10363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.columns[data.isnull().any()].tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hincome', 'treatment', 'vicservices']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HhYjwzD_Qzu",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "*  Here we noticed that there were NaN values for household income, treatment, and vicservices for some of the rows. Instead of filling them with medians, we decided to remove those particular rows entirely because these features were categorical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_mTk27F_WZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove missing valuesl; keep the DataFrame with valid entries in the same variable\n",
        "data.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQK3HH1SAmzq",
        "colab_type": "code",
        "outputId": "ab2dbc1f-cc8c-4550-c217-a7a7287c802c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# return counts for whether or not a crime was reported to the police\n",
        "data['notify'].value_counts(sort=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    5284\n",
              "2    6056\n",
              "Name: notify, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A98RLmRCCTc5",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* The classes were a lot more balanced after all the data cleaning had been done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UStdNmUCZFe",
        "colab_type": "text"
      },
      "source": [
        "##Choosing and Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uYgSE03IkZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop(['notify'], axis=1)\n",
        "y = data['notify']\n",
        "np.random.seed(123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoIwYgDgHeg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [DecisionTreeClassifier(), RandomForestClassifier(n_estimators=200), LogisticRegression(solver='liblinear'), GradientBoostingClassifier(n_estimators=200, learning_rate=1.0), GaussianNB()]\n",
        "names = ['Decision Tree', 'Random Forest', 'Logistic', 'Gradient Boosting', 'Naive Bayes']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qi0xp1KLw5M",
        "colab_type": "code",
        "outputId": "f3868196-8272-4762-b20b-e7456031e48d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "for model, label in zip(models, names):\n",
        "  scores = cross_val_score(model, X, y, cv=KFold(5), scoring='accuracy')\n",
        "  print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.57 (+/- 0.03) [Decision Tree]\n",
            "Accuracy: 0.64 (+/- 0.02) [Random Forest]\n",
            "Accuracy: 0.63 (+/- 0.01) [Logistic]\n",
            "Accuracy: 0.60 (+/- 0.02) [Gradient Boosting]\n",
            "Accuracy: 0.61 (+/- 0.01) [Naive Bayes]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqYhto6tOGpN",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* After using kfold validation on multiple models, we decided to go with the Random Forest Classifier because it had the highest validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI7MCfIZCRvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop(['notify'], axis=1)\n",
        "y = data['notify']\n",
        "np.random.seed(123)\n",
        "X, X_test, y, y_test = train_test_split(X, y,\n",
        "                                                    train_size=0.80, test_size=0.20)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
        "                                                    train_size=0.75, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTmfUiQYH7ga",
        "colab_type": "code",
        "outputId": "a130a01c-965e-4471-ec5f-5fac8d480791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "X_val"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>gender</th>\n",
              "      <th>race1r</th>\n",
              "      <th>hispanic</th>\n",
              "      <th>ethnic1r</th>\n",
              "      <th>ager</th>\n",
              "      <th>marital2</th>\n",
              "      <th>hincome</th>\n",
              "      <th>popsize</th>\n",
              "      <th>region</th>\n",
              "      <th>msa</th>\n",
              "      <th>direl</th>\n",
              "      <th>weapon</th>\n",
              "      <th>weapcat</th>\n",
              "      <th>newcrime</th>\n",
              "      <th>newoff</th>\n",
              "      <th>seriousviolent</th>\n",
              "      <th>injury</th>\n",
              "      <th>treatment</th>\n",
              "      <th>vicservices</th>\n",
              "      <th>locationr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5559</th>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4217</th>\n",
              "      <td>2011</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>2008</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8541</th>\n",
              "      <td>2014</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2648</th>\n",
              "      <td>2010</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11409</th>\n",
              "      <td>2016</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6154</th>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2336</th>\n",
              "      <td>2009</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>2008</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5320</th>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2268 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       year  gender  race1r  ...  treatment  vicservices  locationr\n",
              "5559   2012       1       2  ...        0.0          2.0          3\n",
              "4217   2011       2       1  ...        1.0          2.0          1\n",
              "128    2008       2       1  ...        0.0          2.0          1\n",
              "8541   2014       2       1  ...        0.0          2.0          3\n",
              "2648   2010       2       1  ...        1.0          2.0          5\n",
              "...     ...     ...     ...  ...        ...          ...        ...\n",
              "11409  2016       2       1  ...        0.0          2.0          3\n",
              "6154   2012       2       2  ...        0.0          2.0          1\n",
              "2336   2009       2       2  ...        0.0          2.0          3\n",
              "993    2008       1       1  ...        0.0          2.0          4\n",
              "5320   2012       1       2  ...        0.0          2.0          1\n",
              "\n",
              "[2268 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZcGw3cCiib",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* Here, the data is split into training, validation, and test sets. We allocated 60% of the data for training, 20% for validation, and 20% for testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5gQ20jCfyA",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* Because all of our features are encoded using label encoding, we decided to use random forest because it is not affected by ordinality compared to other learning models such as a neural network. It also yielded the highest validation accuracy amongst other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oYiiLaOCsxW",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjoWrvrUCmRT",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest**\n",
        "* This is a supervised machine learning algorithm that can be used for both regression and classification. It's a predictive modelling tool that uses and ensemble of random individual decision trees. The more decision trees there are in the forest, the more robust the prediction, thus higher the accuracy. It essentially uses a subset of randomly selected features, and the best split feature from the subset is used to split each node, or branch, in a tree.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0ABkeLiQ96O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(3, 100, num = 5)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzcHhH1RulQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid = {'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AiQjUY6iLzF",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* This cell took about 20 minutes to run. You do not have to run it again since we already have the optimal parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbopnWKW6G0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest = RandomForestClassifier(n_estimators=200, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig1g8EhzRjNL",
        "colab_type": "code",
        "outputId": "30995c09-15e8-4e99-a72b-53e7e76f8179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = rforest, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=1, random_state=123, n_jobs = 1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-32-c26903eca541>\", line 2, in <module>\n",
            "    rf_random.fit(X_train, y_train)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\", line 688, in fit\n",
            "    self._run_search(evaluate_candidates)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\", line 1469, in _run_search\n",
            "    random_state=self.random_state))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\", line 667, in evaluate_candidates\n",
            "    cv.split(X, y, groups)))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 1006, in __call__\n",
            "    while self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 834, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 753, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\", line 201, in apply_async\n",
            "    result = ImmediateResult(func)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\", line 582, in __init__\n",
            "    self.results = batch()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 256, in __call__\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 256, in <listcomp>\n",
            "    for func, args, kwargs in self.items]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\", line 516, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\", line 330, in fit\n",
            "    for i, t in enumerate(trees))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 1006, in __call__\n",
            "    while self.dispatch_one_batch(iterator):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 834, in dispatch_one_batch\n",
            "    self._dispatch(tasks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 753, in _dispatch\n",
            "    job = self._backend.apply_async(batch, callback=cb)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\", line 203, in apply_async\n",
            "    callback(result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\", line 338, in __call__\n",
            "    with self.parallel._lock:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/__init__.py\", line 108, in <module>\n",
            "    from tensorflow.contrib import cloud\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/__init__.py\", line 24, in <module>\n",
            "    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cloud/__init__.py\", line 28, in <module>\n",
            "    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/bigtable/__init__.py\", line 29, in <module>\n",
            "    from tensorflow.contrib.bigtable.python.ops.bigtable_api import BigtableClient\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/bigtable/python/ops/bigtable_api.py\", line 44, in <module>\n",
            "    resource_loader.get_path_to_datafile(\"_bigtable.so\"))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/util/loader.py\", line 56, in load_op_library\n",
            "    ret = load_library.load_op_library(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\n",
            "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USuj7C6PR2rn",
        "colab_type": "code",
        "outputId": "bb37fde6-4766-41cd-d043-324bb0d33da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "rf_random.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ea266b0743c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_params_'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfy-pF_kj2E",
        "colab_type": "code",
        "outputId": "e8819b4b-45ca-46ca-89e8-4e52a00aa86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "best_random = rf_random.best_estimator_\n",
        "rf_random_acc = best_random.score(X_val, y_val)\n",
        "print('Accuracy = {:0.4f}%.'.format(rf_random_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0baf3578d51e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrf_random_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy = {:0.4f}%.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_random_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuBC3nRRjuhm",
        "colab_type": "text"
      },
      "source": [
        "**Note:** \n",
        "* These parameters were deemed to perform the best based on the parmeter search. From here, we can narrow down our parameters and do another search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylaJXG3yo77x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [15, 30, 50],\n",
        "    'max_features': ['sqrt'],\n",
        "    'min_samples_leaf': [1, 2, 3],\n",
        "    'min_samples_split': [4, 5, 6],\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NsN6GjQpUDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=200, random_state=123)\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = 1, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Izc8-3pmqR",
        "colab_type": "code",
        "outputId": "fb42ce9b-a56e-45dd-fa75-5197523854f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=15, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, total=   1.1s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=2, min_samples_split=6, total=   1.0s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=4, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=5, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   0.9s\n",
            "[CV] bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6 \n",
            "[CV]  bootstrap=True, max_depth=50, max_features=sqrt, min_samples_leaf=3, min_samples_split=6, total=   1.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
              "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators=200, n_jobs=None,\n",
              "                                              oob_score=False, random_state=123,\n",
              "                                              verbose=0, warm_start=False),\n",
              "             iid='warn', n_jobs=1,\n",
              "             param_grid={'bootstrap': [True], 'max_depth': [15, 30, 50],\n",
              "                         'max_features': ['sqrt'],\n",
              "                         'min_samples_leaf': [1, 2, 3],\n",
              "                         'min_samples_split': [4, 5, 6]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f81wCkQ5lJ5V",
        "colab_type": "code",
        "outputId": "8be07d6b-e0c6-46ae-cf68-9661cfbf845c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_depth': 30,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKkranP1s4Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_grid = grid_search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAa3A0HDkvmQ",
        "colab_type": "code",
        "outputId": "d358515f-cbdd-4a03-fd3b-a7cbf9457df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "base_model = RandomForestClassifier(n_estimators=200, random_state=123)\n",
        "rf_base_model = base_model.fit(X_train, y_train)\n",
        "rf_base_acc = rf_base_model.score(X_val, y_val)\n",
        "rf_grid_acc = best_grid.score(X_val, y_val)\n",
        "print('Base Model Accuracy = {:0.4f}%.'.format(rf_base_acc))\n",
        "print('Random Model Accuracy = {:0.4f}%.'.format(rf_random_acc))\n",
        "print('Grid Search Model Accuracy = {:0.4f}%.'.format(rf_grid_acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base Model Accuracy = 0.6803%.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-dd041ea8ea6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf_grid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Base Model Accuracy = {:0.4f}%.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_base_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Random Model Accuracy = {:0.4f}%.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_random_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Grid Search Model Accuracy = {:0.4f}%.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_grid_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rf_random_acc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nQrDmo0o-8B",
        "colab_type": "text"
      },
      "source": [
        "**Note**\n",
        "\n",
        "\n",
        "*   We noticed that the our validation accuracy rose by .003 after tuning out parameters which is not a lot. But we still decided to go with the model given by our grid search because it did have the highest validation accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIUs2rT6BEth",
        "colab": {}
      },
      "source": [
        "#testing out different max_depths to see which yields the best validation accuracy while not overfitting\n",
        "max_depths = np.linspace(1, 35, 35, endpoint=True)\n",
        "train_results = []\n",
        "val_results = []\n",
        "for max_depth in max_depths:\n",
        "  rf = RandomForestClassifier(max_depth=max_depth, n_jobs=-1, n_estimators=200)\n",
        "  rf.fit(X_train, y_train)\n",
        "  train_results.append(rf.score(X_train, y_train))\n",
        "  val_results.append(rf.score(X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlGUrvjFFh4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(max_depths, train_results)\n",
        "plt.plot(max_depths, val_results, 'r',label='line')\n",
        "plt.legend(['Training','Validation'])\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inNfho2uHCFd",
        "colab_type": "text"
      },
      "source": [
        "**Note**\n",
        "* Here we can see that training accuracy as well as validation accuracy increases as the maximum depth increases. However, bigger values of maximum depth lead to overfitting. The sweet spot seemed to be around 11 according to this graph but our grid search decided that 30 would be most optimal. Due to overfitting, we will proceed with the max_depth value of 11."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dVJA5VeW9Ija",
        "colab": {}
      },
      "source": [
        "best_grid.get_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt5DzgijH741",
        "colab_type": "text"
      },
      "source": [
        "We will use all the parameters given by this except the maximum depth as explained above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqRNoe9d--qn",
        "colab_type": "text"
      },
      "source": [
        "##Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN1HBAjC_5Jj",
        "colab_type": "text"
      },
      "source": [
        "###Permutation Importance Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZYDIovJ-dm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest = RandomForestClassifier(n_estimators=500, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4obhaqs3-9S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm = PermutationImportance(rforest).fit(X_train, y_train)\n",
        "sel = SelectFromModel(perm, threshold=0.01, prefit=True)\n",
        "X_trans_train = sel.transform(X_train)\n",
        "X_trans_val = sel.transform(X_val)\n",
        "X_trans_test = sel.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmGlSQlwALRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm_forest_model = rforest.fit(X_trans_train, y_train)\n",
        "print('Training accuracy: ', perm_forest_model.score(X_trans_train, y_train))\n",
        "print('Validation accuracy: ',perm_forest_model.score(X_trans_val, y_val))\n",
        "eli5.show_weights(perm, feature_names = X_val.columns.to_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8KvwuwPIPQC",
        "colab_type": "text"
      },
      "source": [
        "We were able to get a decent insight on which features were most important in determining whether the person notified the police or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhB5xfTK_8p7",
        "colab_type": "text"
      },
      "source": [
        "### Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKWlgP0nAEkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=200, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "rfe = RFECV(estimator=model, cv=2, scoring='accuracy', n_jobs=-1, verbose=2)\n",
        "rfe.fit(X_train, y_train)\n",
        "print(\"Training accuracy:\", rfe.score(X_train, y_train))\n",
        "print(\"Validation accuracy:\", rfe.score(X_val, y_val))\n",
        "print(\"Number of features:\", rfe.n_features_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C8qhJLG4vDA",
        "colab_type": "text"
      },
      "source": [
        "### Feature Importance Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWwgGCVQ4zC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RandomForestClassifier(n_estimators=500, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "\n",
        "#fit your model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Select the important features of previous model\n",
        "sel = SelectFromModel(model, prefit=True)\n",
        "\n",
        "# Subset features by calling transform on your training X\n",
        "select_X_train = sel.transform(X_train)\n",
        "select_X_test = sel.transform(X_test)\n",
        "select_X_val = sel.transform(X_val)\n",
        "# We want to create a train model\n",
        "sel_model = RandomForestClassifier(n_estimators=500, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "sel_model.fit(select_X_train, y_train)\n",
        "print('Training Accuracy: ', sel_model.score(select_X_train, y_train))\n",
        "print('Validation Accuracy: ', sel_model.score(select_X_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMy7DwJE-oTq",
        "colab_type": "text"
      },
      "source": [
        "Feature importance selection resulted in a much lower validation score compared to other feature selection methods so we decided not to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fz8AMLMKn8A",
        "colab_type": "text"
      },
      "source": [
        "###Feature Selection Based on Correlation and Intuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTZFle6hLYNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corrmat = data.corr()\n",
        "plt.figure(figsize=(17,17))\n",
        "g = sns.heatmap(corrmat, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOGsMrVZLbgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns with at least -.1 correlation\n",
        "corrmat.loc[corrmat['notify'] < -0.1]['notify']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZrre8XLduJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# columns with at least .1 correlation\n",
        "corrmat.loc[corrmat['notify'] > 0.1]['notify']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ35c7hWLkH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating feature list\n",
        "neg = corrmat.loc[corrmat['notify'] < -0.1].index.tolist()\n",
        "pos = corrmat.loc[corrmat['notify'] > 0.1].index.tolist()\n",
        "pos.pop(0)\n",
        "pos.extend(neg)\n",
        "pos.extend(['race1r','year'])\n",
        "features = pos\n",
        "features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQR6mcWeLo3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_feats = X_train[features]\n",
        "X_val_feats = X_val[features]\n",
        "X_test_feats = X_test[features]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfGy-45_Nl7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest_feats = RandomForestClassifier(bootstrap= True, max_depth=11, max_features = 'sqrt', min_samples_leaf=2, min_samples_split=2, n_estimators=500)\n",
        "rforest_feats.fit(X_train_feats, y_train)\n",
        "rforest_feats_test_pred = rforest_feats.predict(X_test_feats)\n",
        "print('Training Accuracy: ', rforest_feats.score(X_train_feats, y_train))\n",
        "print('Validation Accuracy: ', rforest_feats.score(X_val_feats, y_val))\n",
        "confusion_matrix(y_test, rforest_feats_test_pred, labels=[1,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar167FdcNzUr",
        "colab_type": "text"
      },
      "source": [
        "Feature selection based on correlation and intuition did not really give us better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQEYMYR-m2m",
        "colab_type": "text"
      },
      "source": [
        "##One Hot Encoding all Categorical Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irruqcSKA9ZC",
        "colab_type": "text"
      },
      "source": [
        "We wanted to explore if one hot encoding all the categorical variables would do anything to help the accuracy and maybe give more insight as to what features are affecting the model the most."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw2Lg1779NB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_hot = pd.get_dummies(data, columns=data.columns.to_list())\n",
        "data_hot.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneTy2IFzMCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_hot.drop(['notify_1', 'notify_2'], axis=1)\n",
        "y = data['notify']\n",
        "np.random.seed(123)\n",
        "X_hot, X_test_hot, y_hot, y_test_hot = train_test_split(X, y,\n",
        "                                                    train_size=0.80, test_size=0.20)\n",
        "X_train_hot, X_val_hot, y_train_hot, y_val_hot = train_test_split(X, y,\n",
        "                                                    train_size=0.75, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSkYYY1HzOnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot = RandomForestClassifier(bootstrap= True, max_depth=11, max_features = 'sqrt', min_samples_leaf=2, min_samples_split=2, n_estimators=2000)\n",
        "one_hot.fit(X_train_hot, y_train_hot)\n",
        "one_hot.score(X_val_hot, y_val_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk00wL3Bzg62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances = pd.DataFrame(one_hot.feature_importances_[-15:],\n",
        "                                   index = X_train_hot.columns[-15:],\n",
        "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
        "feature_importances.plot.bar(figsize=(10,10))\n",
        "plt.title('RandomForest')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEUWjb3zN43H",
        "colab_type": "text"
      },
      "source": [
        "###Final Model\n",
        "* We decided to use permutation selection to pick our features as well as model because it is supposed to not choose features based on high cardinality and ignore ordinality as opposed to traditional feature importance selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaRhtUTMDZzu",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo7NMb6bDhpw",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VroS0gW7RdtQ",
        "colab_type": "text"
      },
      "source": [
        "####Confusion Matrix and Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmtHpVPcOrPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_df_cm = pd.DataFrame(confusion_matrix(y_test, perm_forest_model.predict(X_trans_test)), range(2), range(2))\n",
        "nb_df_cm = nb_df_cm.rename(index=str, columns={0: \"yes\", 1: \"no\"})\n",
        "nb_df_cm.index = ['yes', 'no'] \n",
        "plt.figure(figsize = (10,7))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(nb_df_cm, \n",
        "           annot=True,\n",
        "           annot_kws={\"size\": 16},\n",
        "           fmt='g')\n",
        "\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wPpM5V8PCBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm_test_acc = perm_forest_model.score(X_trans_test, y_test)\n",
        "print('Random Forest Test Accuracy: ', perm_test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl4AS8XXRh4f",
        "colab_type": "text"
      },
      "source": [
        "#### Feature Importance Explanations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7OAJtmOuX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eli5.show_weights(perm, feature_names = X_val.columns.to_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m39VxQtdPSDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest = RandomForestClassifier(n_estimators=500, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "rforest.fit(X_train, y_train)\n",
        "feature_importances = pd.DataFrame(rforest.feature_importances_,\n",
        "                                   index = X_train.columns,\n",
        "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
        "feature_importances.plot.bar(figsize=(8,8))\n",
        "plt.title('RandomForest Feature Importance')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re9naX4NQmGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances = pd.DataFrame(one_hot.feature_importances_[-15:],\n",
        "                                   index = X_train_hot.columns[-15:],\n",
        "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
        "feature_importances.plot.bar(figsize=(10,10))\n",
        "plt.title('One Hot Encoding Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f7NiB2LQQOk",
        "colab_type": "text"
      },
      "source": [
        "##Note##\n",
        "Both feature importance models showed that 'locationr', the location of the incident, was considered the most important feature. When one hot encoding was used to train the model, it also showed the location was the most important feature. More specifically location 1 and 4 seemed to have the most impact which makes sense because location 1 and 4 represent \"At or near the victim's home\" and \"school\" respectively. Although permutation importance was supposed to reduce the bias that gets introduced with high cardinality categorical variables, it seems like it failed to do that since year and age were still considered pretty improtant features and they had the one of the highest cardinalities amongst all of the other variables. It is pretty evident that one hot encoding most accurately gave us which features were actually important althought it did not perform the best.\n",
        "\n",
        "It was interesting to see that race was actually considered to be one of the more useless features. It was not too surprising though because in our EDA we saw that the rates at which the two races report to the police was about the same. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIh6k_dFU9dg",
        "colab_type": "text"
      },
      "source": [
        "##Extra Classification (Race)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFoMnd8DVBJa",
        "colab_type": "text"
      },
      "source": [
        "We tried predicting the race based on other features but we were not able to reach a meaningful conclusion because our model predicted white for most cases. This is probably due to the huge class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jch22DagTVyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzwNHz21Pg29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_hot = pd.get_dummies(data, columns=data.columns.to_list())\n",
        "data_hot.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH36wVkVUQeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data_hot.drop(['race1r_1', 'race1r_2', 'race1r_3', 'hispanic_1', 'hispanic_2', 'ethnic1r_1', 'ethnic1r_2', 'ethnic1r_3',\n",
        "       'ethnic1r_4'], axis=1)\n",
        "y = data['race1r']\n",
        "np.random.seed(123)\n",
        "X_hot, X_test_hot, y_hot, y_test_hot = train_test_split(X, y,\n",
        "                                                    train_size=0.80, test_size=0.20)\n",
        "X_train_hot, X_val_hot, y_train_hot, y_val_hot = train_test_split(X, y,\n",
        "                                                    train_size=0.75, test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JogD9VCnSyUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rforest = RandomForestClassifier(n_estimators=500, max_depth=11, bootstrap=True, max_features='sqrt', min_samples_leaf=1, min_samples_split=4, random_state=123)\n",
        "rforest.fit(X_train_hot, y_train_hot)\n",
        "print(rforest.score(X_train_hot, y_train_hot))\n",
        "print(rforest.score(X_val_hot, y_val_hot))\n",
        "rforest.score(X_test_hot, y_test_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q33SnlaETCHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_df_cm = pd.DataFrame(confusion_matrix(y_test_hot, rforest.predict(X_test_hot)), range(3), range(3))\n",
        "nb_df_cm = nb_df_cm.rename(index=str, columns={0: \"White\", 1: \"Black\", 2: 'Other'})\n",
        "nb_df_cm.index = ['White', 'Black', 'Other'] \n",
        "plt.figure(figsize = (10,7))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(nb_df_cm, \n",
        "           annot=True,\n",
        "           annot_kws={\"size\": 16},\n",
        "           fmt='g')\n",
        "\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoMqL3q4TxXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importances = pd.DataFrame(rforest.feature_importances_[-15:],\n",
        "                                   index = X_train_hot.columns[-15:],\n",
        "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
        "feature_importances.plot.bar(figsize=(10,10))\n",
        "plt.title('One Hot Encoding Importances')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNqt_KOtV-hI",
        "colab_type": "text"
      },
      "source": [
        "**Reflection:**\n",
        "\n",
        "* While analyzing the data, it was quickly realized early on that race has very little impact in influencing whether or not a victim reports a crime. As briefly mentioned previously, this may be due to the disparity in the number of white households (approximately 12,000) surveyed versus the number of black households surveyed (approximately 2000). Even with this difference, it appears that the black race report crimes more often than whites, which contradicts our initial belief that African Americans are less likely to report crimes/victimization instances. Given this, perhaps blacks experience crimes that are worse than those experienced by whites, hencing blacks report more despite the difference in numbers in each race. Nevertheless, this served as an important note to recognize during our analysis.\n",
        "\n",
        "* After analyzing the data, we concluded that the location of incident (locationr) is an important feature. Location is a measure of where the victimization occurred, and it includes: 1) at or near victim’s home; 2) at or near friend, neighbor, or relative’s home; 3) commercial place, parking lot, or other public area; 4) school; and 5) other location. This finding provides an crucial indicator of where crime problems are occurring, therefore drawing attention to what about those locations attract criminal activity and what can be done to combat them. Perhaps some attribute or attributes about a particular type of location have the propensity to make them more prone to criminal activity, therefore this suggests changes to be made.\n",
        "\n",
        "* We used the random forest learning algorithm for feature selection, but we eventually decided to also explore the usage of one hot encoding. Essentially, one hot encoding transforms categorical data into a format that is easier for machine learning algorithms to work with. We also used permuation importance feature selection to give us a glimpse of which features may be most important in determining whether a victim notified the police or not in regard to their victimization. As mentioned above, both feature importance models indicate that location of the incident is the most crucial feature. One hot encoding ultimately provided a more accurate prediction of which variables are important. Although this was not the best model for our study, it nevertheless allowed us to inspect the significant variables of the dataset. Therefore, perhaps another research could further investigate in the future.\n",
        "\n",
        "* The conclusion of our research simply gives a small glimpse of the possibilities that can come from the dataset, for there are many other goals that can be achieved by exploring it further.\n",
        "From a legal perspective, the this dataset and the continuous collection of responses to this survey could help serve as an index of any changes in reporting behavior in the population. This could then help shape the legal aspect of the criminal justice system, such as enacting new laws or creating programs that could improve behavior based on legal consequences. From a policy perspective, the dataset and future areas of research could help explain any issues with the criminal justice system. By determining such problems, it forces public policy to change in order to reflect the areas that need attention in this country. There are reasons behind people’s decisions to report crimes or not, there are reasons why particular locations experience more crime, there are reasons behind the types of crimes, and much. Therefore, it’s worth looking into channels that may influence policy making to better the system. From an ethical perspective, public confidence in police is crucial to the wellbeing of a society. Future areas of research could explore if perhaps police training meet the ethical and moral standards that reflect this country. We often hear about police brutality and the police fatally killing allegedly innocent people, so perhaps there is a discrepancy between their training and our ethical standards. Perhaps by looking into victims’ accounts of their victimization and their process with the police after the fact could shine some light onto the situation.  There remains much room for improvement and exploration after our research, but ultimately, this dataset is optimal to provide information of the nature and levels of crime that occur in our society.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh4DCj2wWkmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}